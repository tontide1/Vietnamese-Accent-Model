{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288ae215",
   "metadata": {},
   "source": [
    "# Vietnamese Accent Model - Colab Notebook\n",
    "\n",
    "This notebook contains the entire Vietnamese Accent Model project code, adapted to run on Google Colab. It includes all functionality from the original project files:\n",
    "\n",
    "- Data loading and preprocessing\n",
    "- Model training\n",
    "- Accent prediction\n",
    "- Model evaluation\n",
    "\n",
    "Original source files:\n",
    "- `utils/model_trainer.py`\n",
    "- `utils/data_loader.py`\n",
    "- `utils/utils.py`\n",
    "- `utils/predictor.py`\n",
    "- `main.py`\n",
    "- `train_model.py`\n",
    "- `evaluate_model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44e387",
   "metadata": {},
   "source": [
    "## 0. Setup Environment\n",
    "\n",
    "First, let's install the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddb787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: dill~=0.3.7 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: pandas~=2.0.3 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: matplotlib~=3.7.5 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: seaborn~=0.13.2 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm~=4.66.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: requests~=2.31.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: click in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk==3.5) (8.2.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk==3.5) (1.5.0)\n",
      "Requirement already satisfied: regex in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk==3.5) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn==1.2.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn==1.2.2) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn==1.2.2) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from pandas~=2.0.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from pandas~=2.0.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from pandas~=2.0.3) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from matplotlib~=3.7.5) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm~=4.66.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from requests~=2.31.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from requests~=2.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from requests~=2.31.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from requests~=2.31.0) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tontide1\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from python-dateutil>=2.8.2->pandas~=2.0.3) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tontide1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install nltk==3.5 scikit-learn==1.2.2 dill~=0.3.7 pandas~=2.0.3 matplotlib~=3.7.5 seaborn~=0.13.2 tqdm~=4.66.0 requests~=2.31.0\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738122b",
   "metadata": {},
   "source": [
    "## 1. Directory Structure Setup\n",
    "\n",
    "Next, let's set up the directory structure for our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c2e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directories created:\n",
      "- Data dir: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\n",
      "- Model dir: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\n",
      "- Plots dir: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create project directories\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project directories created:\")\n",
    "print(f\"- Data dir: {DATA_DIR}\")\n",
    "print(f\"- Model dir: {MODEL_DIR}\")\n",
    "print(f\"- Plots dir: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fa709",
   "metadata": {},
   "source": [
    "## 2. Define Utility Functions (utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20e9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize example: ['đây_là', 'một', 'câu', 'ví', 'dụ']\n",
      "Remove accent example: hoang\n"
     ]
    }
   ],
   "source": [
    "# Porting utils.py\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n",
    "VN_SYLLABLES_FILE_PATH = os.path.join(DATA_DIR, \"vn_syllables.txt\")\n",
    "\n",
    "def tokenize(doc: str) -> list[str]:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    # Allow underscore, remove other punctuation\n",
    "    table = str.maketrans('', '', string.punctuation.replace(\"_\", \"\"))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word]  # Remove empty strings\n",
    "    return tokens\n",
    "\n",
    "def remove_vn_accent(word: str) -> str:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[áàảãạăắằẳẵặâấầẩẫậ]', 'a', word)\n",
    "    word = re.sub(r'[éèẻẽẹêếềểễệ]', 'e', word)\n",
    "    word = re.sub(r'[óòỏõọôốồổỗộơớờởỡợ]', 'o', word)\n",
    "    word = re.sub(r'[íìỉĩị]', 'i', word)\n",
    "    word = re.sub(r'[úùủũụưứừửữự]', 'u', word)\n",
    "    word = re.sub(r'[ýỳỷỹỵ]', 'y', word)\n",
    "    word = re.sub(r'đ', 'd', word)\n",
    "    return word\n",
    "\n",
    "def gen_accents_word(word: str, syllables_path: str = VN_SYLLABLES_FILE_PATH) -> set[str]:\n",
    "    \"\"\"\n",
    "    Sinh văn bản tự động với file vn_syllables.txt\n",
    "    \"\"\"\n",
    "    normalized_input_word = word.lower()\n",
    "    word_no_accent = remove_vn_accent(normalized_input_word)\n",
    "    all_accent_word = {normalized_input_word}  # Start with the input word (normalized)\n",
    "\n",
    "    if not os.path.exists(syllables_path):\n",
    "        print(f\"Warning: Syllables file not found at {syllables_path}. \"\n",
    "              f\"Accent generation will be limited to the input word: '{word}'.\")\n",
    "        if word_no_accent != normalized_input_word:\n",
    "            all_accent_word.add(word_no_accent)\n",
    "        return all_accent_word\n",
    "\n",
    "    try:\n",
    "        with open(syllables_path, 'r', encoding='utf-8') as f:\n",
    "            for w_line in f.read().splitlines():\n",
    "                w_line_lower = w_line.lower()  # Normalize file content\n",
    "                if remove_vn_accent(w_line_lower) == word_no_accent:\n",
    "                    all_accent_word.add(w_line_lower)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing syllables file {syllables_path}: {e}\")\n",
    "    \n",
    "    return all_accent_word\n",
    "\n",
    "# Test the utility functions\n",
    "print(\"Tokenize example:\", tokenize(\"Đây_là một câu, ví dụ.\"))\n",
    "print(\"Remove accent example:\", remove_vn_accent(\"hoàng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b20b2",
   "metadata": {},
   "source": [
    "## 3. Define Data Loader (data_loader.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8007d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porting data_loader.py\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Data paths configuration\n",
    "TRAIN_ZIP_URL = \"https://github.com/hoanganhpham1006/Vietnamese_Language_Model/raw/master/Train_Full.zip\"\n",
    "TRAIN_ZIP_NAME = \"Train_Full.zip\"\n",
    "TRAIN_ZIP_PATH = os.path.join(DATA_DIR, TRAIN_ZIP_NAME)\n",
    "TRAIN_EXTRACT_PATH = os.path.join(DATA_DIR, \"Train_Full\")\n",
    "\n",
    "SYLLABLES_URL = \"https://gist.githubusercontent.com/hieuthi/0f5adb7d3f79e7fb67e0e499004bf558/raw/135a4d9716e49a981624474156d6f247b9b46f6a/all-vietnamese-syllables.txt\"\n",
    "SYLLABLES_NAME = \"vn_syllables.txt\"\n",
    "SYLLABLES_PATH = os.path.join(DATA_DIR, SYLLABLES_NAME)\n",
    "\n",
    "def download_file(url, local_path, desc=\"Downloading file\"):\n",
    "    \"\"\"\n",
    "    Download a file from the given URL to the specified local path.\n",
    "    \"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        chunk_size = 8192\n",
    "        \n",
    "        with open(local_path, 'wb') as f, tqdm(\n",
    "            desc=desc,\n",
    "            total=total_size,\n",
    "            unit='B', unit_scale=True, unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                size = f.write(chunk)\n",
    "                bar.update(size)\n",
    "    return local_path\n",
    "\n",
    "def download_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Download and extract the training data and syllables file.\n",
    "    \"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # Download syllables file\n",
    "    if not os.path.exists(SYLLABLES_PATH):\n",
    "        print(f\"Downloading Vietnamese syllables file to {SYLLABLES_PATH}...\")\n",
    "        download_file(SYLLABLES_URL, SYLLABLES_PATH, desc=\"Downloading syllables file\")\n",
    "    else:\n",
    "        print(f\"Syllables file already exists at {SYLLABLES_PATH}\")\n",
    "    \n",
    "    # Download training data\n",
    "    if not os.path.exists(TRAIN_EXTRACT_PATH) or not os.listdir(TRAIN_EXTRACT_PATH):\n",
    "        if not os.path.exists(TRAIN_ZIP_PATH):\n",
    "            print(f\"Downloading training data to {TRAIN_ZIP_PATH}...\")\n",
    "            download_file(TRAIN_ZIP_URL, TRAIN_ZIP_PATH, desc=\"Downloading training data\")\n",
    "        \n",
    "        print(f\"Extracting training data to {TRAIN_EXTRACT_PATH}...\")\n",
    "        os.makedirs(TRAIN_EXTRACT_PATH, exist_ok=True)\n",
    "        with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_DIR)\n",
    "    else:\n",
    "        print(f\"Training data already exists at {TRAIN_EXTRACT_PATH}\")\n",
    "    \n",
    "    return check_data_exists()\n",
    "\n",
    "def check_data_exists():\n",
    "    \"\"\"kiểm tra xem dữ liệu đã có chưa\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Data directory configured at: {DATA_DIR}\")\n",
    "    all_data_exists = True\n",
    "    if not os.path.exists(TRAIN_EXTRACT_PATH) or not os.listdir(TRAIN_EXTRACT_PATH):\n",
    "        print(f\"Warning: Training data directory {TRAIN_EXTRACT_PATH} is missing or empty.\")\n",
    "        print(\"Please ensure you have downloaded and extracted 'Train_Full.zip' into the 'data/Train_Full' directory.\")\n",
    "        all_data_exists = False\n",
    "    else:\n",
    "        print(f\"Training data found at: {TRAIN_EXTRACT_PATH}\")\n",
    "\n",
    "    if not os.path.exists(SYLLABLES_PATH):\n",
    "        print(f\"Warning: Vietnamese syllables file {SYLLABLES_PATH} is missing.\")\n",
    "        print(\"Please ensure you have 'vn_syllables.txt' in the 'data' directory.\")\n",
    "        all_data_exists = False\n",
    "    else:\n",
    "        print(f\"Vietnamese syllables file found at: {SYLLABLES_PATH}\")\n",
    "    return all_data_exists\n",
    "\n",
    "def load_corpus(data_extract_path: str = TRAIN_EXTRACT_PATH) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    load dữ liệu từ thư mục đã giải nén\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_extract_path):\n",
    "        print(f\"Error: Training data path not found: {data_extract_path}\")\n",
    "        print(\"Please run download_and_prepare_data() first or ensure data is correctly placed.\")\n",
    "        return []\n",
    "\n",
    "    full_text_content = []\n",
    "    print(f\"Loading corpus from: {data_extract_path}\")\n",
    "\n",
    "    for dirname, _, filenames in os.walk(data_extract_path):\n",
    "        for filename in tqdm(filenames, desc=f\"Reading files in {os.path.basename(dirname)}\"):\n",
    "            if filename.endswith(\".txt\"): \n",
    "                try:\n",
    "                    with open(os.path.join(dirname, filename), 'r', encoding='UTF-16') as f:\n",
    "                        full_text_content.append(f.read())\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read file {os.path.join(dirname, filename)}: {e}\")\n",
    "    \n",
    "    if not full_text_content:\n",
    "        print(\"No text files found or loaded from the training data path.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Loaded {len(full_text_content)} documents.\")\n",
    "    full_data_string = \". \".join(full_text_content)\n",
    "    full_data_string = full_data_string.replace(\"\\n\", \". \")\n",
    "    \n",
    "    corpus = []\n",
    "    raw_sents = re.split(r'[.?!]\\s+', full_data_string) \n",
    "    print(f\"Processing {len(raw_sents)} raw sentences...\")\n",
    "    for sent in tqdm(raw_sents, desc=\"Tokenizing sentences\"):\n",
    "        if sent.strip(): # Ensure sentence is not just whitespace\n",
    "            corpus.append(tokenize(sent))\n",
    "    \n",
    "    print(f\"Corpus created with {len(corpus)} tokenized sentences.\")\n",
    "    return corpus\n",
    "\n",
    "# Hàm tận dụng multiprocessing để xử lý nhiều câu cùng lúc\n",
    "# cải thiện tốc xử lý cho các tác vụ nặng như tokenize\n",
    "def _process_single_sentence_for_splitting(sent_accented: str):\n",
    "    if sent_accented.strip():\n",
    "        temp_tokenized_for_unaccenting = tokenize(sent_accented)\n",
    "        unaccented_words = [remove_vn_accent(word) for word in temp_tokenized_for_unaccenting]\n",
    "        unaccented_sentence_str = \" \".join(unaccented_words)\n",
    "        tokenized_accented_sentence = tokenize(sent_accented)\n",
    "        if unaccented_sentence_str and tokenized_accented_sentence:\n",
    "            return (unaccented_sentence_str, tokenized_accented_sentence)\n",
    "    return None\n",
    "\n",
    "def load_and_split_corpus(data_extract_path: str = TRAIN_EXTRACT_PATH, test_size: float = 0.2, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load dữ liệu, tạo phiên bản không dấu, tokenize câu có dấu,\n",
    "    và chia thành tập huấn luyện và tập kiểm thử.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_extract_path):\n",
    "        print(f\"Error: Training data path not found: {data_extract_path}\")\n",
    "        print(\"Please run download_and_prepare_data() first or ensure data is correctly placed.\")\n",
    "        return [], []\n",
    "\n",
    "    full_text_content = []\n",
    "    print(f\"Loading corpus from: {data_extract_path}\")\n",
    "    for dirname, _, filenames in os.walk(data_extract_path):\n",
    "        for filename in tqdm(filenames, desc=f\"Reading files in {os.path.basename(dirname)}\"):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                try:\n",
    "                    with open(os.path.join(dirname, filename), 'r', encoding='UTF-16') as f:\n",
    "                        full_text_content.append(f.read())\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read file {os.path.join(dirname, filename)}: {e}\")\n",
    "    \n",
    "    if not full_text_content:\n",
    "        print(\"No text files found or loaded from the training data path.\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"Loaded {len(full_text_content)} documents.\")\n",
    "    full_data_string = \". \".join(full_text_content)\n",
    "    full_data_string = full_data_string.replace(\"\\n\", \". \")\n",
    "    \n",
    "    raw_sentences_with_accent = re.split(r'[.?!]\\s+', full_data_string)\n",
    "\n",
    "    # GIỚI HẠN SỐ LƯỢNG CÂU ĐỂ TRÁNH MemoryError\n",
    "    MAX_PROCESS_SENTENCES = 100\n",
    "    if len(raw_sentences_with_accent) > MAX_PROCESS_SENTENCES:\n",
    "        print(f\"CẢNH BÁO: Giới hạn xử lý {MAX_PROCESS_SENTENCES} câu đầu tiên trên tổng số {len(raw_sentences_with_accent)} câu để tiết kiệm bộ nhớ.\")\n",
    "        raw_sentences_with_accent = raw_sentences_with_accent[:MAX_PROCESS_SENTENCES]\n",
    "    \n",
    "    print(f\"Processing {len(raw_sentences_with_accent)} raw sentences for splitting using multiprocessing...\")\n",
    "    \n",
    "    with mp.Pool() as pool:\n",
    "        num_cores = os.cpu_count() or 1 # Đảm bảo num_cores ít nhất là 1\n",
    "        chunk_size = max(1, len(raw_sentences_with_accent) // (num_cores * 4))\n",
    "        \n",
    "        results_iterator = pool.imap(_process_single_sentence_for_splitting, \n",
    "                                     tqdm(raw_sentences_with_accent, desc=\"Generating unaccented and tokenizing (parallel)\"),\n",
    "                                     chunksize=chunk_size)\n",
    "        \n",
    "        # Lọc bỏ các kết quả None (nếu câu rỗng hoặc không xử lý được)\n",
    "        processed_sentences = [res for res in results_iterator if res is not None]\n",
    "\n",
    "    if not processed_sentences:\n",
    "        print(\"No sentences could be processed. Check data and tokenization.\")\n",
    "        return [], []\n",
    "        \n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(processed_sentences)\n",
    "    \n",
    "    split_index = int(len(processed_sentences) * (1 - test_size))\n",
    "    train_data_pairs = processed_sentences[:split_index]\n",
    "    test_data_pairs = processed_sentences[split_index:]\n",
    "    \n",
    "    train_corpus = [pair[1] for pair in train_data_pairs] # Chỉ lấy câu có dấu đã tokenize cho training\n",
    "    test_set = test_data_pairs # Giữ nguyên (câu không dấu, câu có dấu tokenize) cho testing\n",
    "    \n",
    "    print(f\"Data split: {len(train_corpus)} training sentences, {len(test_set)} test sentences.\")\n",
    "    return train_corpus, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e99734",
   "metadata": {},
   "source": [
    "## 4. Define Model Trainer (model_trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a287d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porting model_trainer.py\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "DEFAULT_MODEL_FILENAME = \"kneserney_trigram_model.pkl\"\n",
    "N_GRAM_ORDER = 3\n",
    "\n",
    "def train_ngram_model(corpus: list[list[str]],\n",
    "                      n: int = N_GRAM_ORDER): # Signature changed, model_class removed\n",
    "    if not corpus:\n",
    "        print(\"Corpus is empty. Cannot train model.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing data for custom {n}-gram model...\")\n",
    "\n",
    "    start_symbol = \"<s>\"\n",
    "    end_symbol = \"</s>\"\n",
    "\n",
    "    ngram_counts = defaultdict(int)\n",
    "    context_counts = defaultdict(int) # For (n-1)-grams\n",
    "    vocab = set()\n",
    "\n",
    "    for sentence_tokens in corpus:\n",
    "        # Pad sentence: (n-1) start symbols, 1 end symbol.\n",
    "        # e.g., for n=3 (trigram): ['<s>', '<s>'] + sentence_tokens + ['</s>']\n",
    "        # e.g., for n=1 (unigram): [] + sentence_tokens + ['</s>']\n",
    "        current_padded_sentence = ([start_symbol] * (n - 1)) + sentence_tokens + [end_symbol]\n",
    "\n",
    "        for token in sentence_tokens: # Original sentence tokens for vocab\n",
    "            vocab.add(token)\n",
    "\n",
    "        # Generate n-grams and (n-1)-gram contexts\n",
    "        # Iterate up to the point where the last n-gram can be formed\n",
    "        for i in range(len(current_padded_sentence) - n + 1):\n",
    "            ngram = tuple(current_padded_sentence[i : i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "\n",
    "            if n > 1:\n",
    "                # The context is the first (n-1) tokens of the n-gram\n",
    "                context = tuple(current_padded_sentence[i : i + n - 1])\n",
    "                context_counts[context] += 1\n",
    "            # For n=1 (unigrams), context_counts will be handled later if needed for P(w) = count(w)/TotalWords\n",
    "\n",
    "    # For unigram model (n=1), if we define P(w) = count(w) / total_tokens,\n",
    "    # context_counts can store the total number of tokens.\n",
    "    if n == 1:\n",
    "        total_word_occurrences = sum(ngram_counts.values()) # Sum of counts of all unigrams (word occurrences)\n",
    "        if total_word_occurrences > 0:\n",
    "            context_counts[()] = total_word_occurrences # Global context for unigrams is the total count\n",
    "\n",
    "    print(f\"Custom model training complete. Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Number of unique {n}-grams: {len(ngram_counts)}\")\n",
    "    if n > 1 or (n == 1 and context_counts):\n",
    "        print(f\"Number of unique contexts: {len(context_counts)}\")\n",
    "\n",
    "    # The \"model\" is now a dictionary of these counts, vocab, and n\n",
    "    custom_model = {\n",
    "        \"n\": n,\n",
    "        \"vocab\": list(vocab), # Store as list for potential JSON needs, pickle handles sets\n",
    "        \"ngram_counts\": dict(ngram_counts), # Convert defaultdict to dict\n",
    "        \"context_counts\": dict(context_counts), # Convert defaultdict to dict\n",
    "    }\n",
    "    return custom_model\n",
    "\n",
    "def save_model(model, \n",
    "               model_dir: str = MODEL_DIR, \n",
    "               filename: str = DEFAULT_MODEL_FILENAME):\n",
    "    if model is None:\n",
    "        print(\"Model is None. Nothing to save.\")\n",
    "        return False\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, filename)\n",
    "    print(f\"Saving model to {model_path}...\")\n",
    "    try:\n",
    "        with open(model_path, 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "        print(f\"Model successfully saved to {model_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {model_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0fae7",
   "metadata": {},
   "source": [
    "## 5. Define Predictor (predictor.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4de3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porting predictor.py\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "DEFAULT_MODEL_PATH = os.path.join(MODEL_DIR, DEFAULT_MODEL_FILENAME)\n",
    "\n",
    "# def load_model(model_path: str = DEFAULT_MODEL_PATH):\n",
    "#     if not os.path.exists(model_path):\n",
    "#         print(f\"ERROR: Model file not found at {model_path}\")\n",
    "#         print(f\"Please train and save the model first or provide a valid path.\")\n",
    "#         return None\n",
    "    \n",
    "#     print(f\"Attempting to load model from {model_path}...\")\n",
    "#     try:\n",
    "#         with open(model_path, 'rb') as fin:\n",
    "#             model_loaded = pickle.load(fin)\n",
    "#         # Kiểm tra sơ bộ xem có thuộc tính vocab không, vì các model NLTK thường có\n",
    "#         if hasattr(model_loaded, 'vocab'):\n",
    "#             print(f\"Model loaded successfully from {model_path}. Vocabulary size: {len(model_loaded.vocab)}\")\n",
    "#         else:\n",
    "#             print(f\"Model loaded from {model_path}, but it does not seem to have a 'vocab' attribute. Type: {type(model_loaded)}\")\n",
    "#         return model_loaded\n",
    "#     except FileNotFoundError: # Để chắc chắn, dù đã kiểm tra ở trên\n",
    "#         print(f\"CRITICAL ERROR: Model file not found during open or pickle.load operation: {model_path}.\")\n",
    "#     except pickle.UnpicklingError as e_pickle:\n",
    "#         print(f\"CRITICAL ERROR: Could not unpickle model from {model_path}. File might be corrupted or not a NLTK pickle file. Details: {e_pickle}\")\n",
    "#     except AttributeError as e_attr:\n",
    "#         print(f\"CRITICAL ERROR: AttributeError during unpickling model from {model_path}. This might indicate a mismatch in class definitions (e.g., model saved with a different NLTK version or custom class not found). Details: {e_attr}\")\n",
    "#     except ImportError as e_imp:\n",
    "#         print(f\"CRITICAL ERROR: ImportError during unpickling model from {model_path}. A custom class definition might be missing. Details: {e_imp}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"CRITICAL ERROR: An unexpected error occurred while loading the model from {model_path}. Details: {e}\")\n",
    "#     return None\n",
    "\n",
    "def load_model(model_path: str = DEFAULT_MODEL_PATH):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERROR: Model file not found at {model_path}\")\n",
    "        print(f\"Please train and save the model first or provide a valid path.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Attempting to load model from {model_path}...\")\n",
    "    try:\n",
    "        with open(model_path, 'rb') as fin:\n",
    "            model_loaded = pickle.load(fin)\n",
    "        \n",
    "        # Sửa đoạn kiểm tra này để phù hợp với mô hình từ điển\n",
    "        if isinstance(model_loaded, dict) and \"vocab\" in model_loaded:\n",
    "            print(f\"Model loaded successfully from {model_path}. Vocabulary size: {len(model_loaded['vocab'])}\")\n",
    "        elif hasattr(model_loaded, 'vocab'):\n",
    "            print(f\"Model loaded successfully from {model_path}. Vocabulary size: {len(model_loaded.vocab)}\")\n",
    "        else:\n",
    "            print(f\"Model loaded from {model_path}, but it does not seem to have a 'vocab' attribute or key. Type: {type(model_loaded)}\")\n",
    "        \n",
    "        return model_loaded\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Could not load model from {model_path}. Details: {e}\")\n",
    "        return None\n",
    "\n",
    "_detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# NLTK-like Language Model class implementation for our custom model\n",
    "class CustomNGramModel:\n",
    "    def __init__(self, model_dict):\n",
    "        self.order = model_dict[\"n\"]\n",
    "        self.vocab = set(model_dict[\"vocab\"])\n",
    "        self.ngram_counts = model_dict[\"ngram_counts\"]\n",
    "        self.context_counts = model_dict[\"context_counts\"]\n",
    "        \n",
    "    def logscore(self, word, context=None):\n",
    "        \"\"\"Return the log probability of the word given the context\"\"\"\n",
    "        import math\n",
    "        if context is None:\n",
    "            context = tuple()\n",
    "        \n",
    "        # Ensure context is a tuple of the right length\n",
    "        if len(context) > self.order - 1:\n",
    "            context = context[-(self.order - 1):]\n",
    "        \n",
    "        # Make sure the tuple is the right length by padding if needed\n",
    "        if len(context) < self.order - 1:\n",
    "            context = tuple([\"<s>\"] * ((self.order - 1) - len(context))) + tuple(context)\n",
    "        \n",
    "        # Check for the n-gram and get its count\n",
    "        ngram = context + (word,)\n",
    "        ngram_count = self.ngram_counts.get(ngram, 0)\n",
    "        \n",
    "        # Get the context count\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        \n",
    "        # Calculate probability with smoothing\n",
    "        if context_count == 0:\n",
    "            # Backed off to a very small probability\n",
    "            return math.log(1e-10)\n",
    "        \n",
    "        # Use simple MLE probability here (could be improved with real smoothing)\n",
    "        prob = (ngram_count + 1e-10) / (context_count + 1e-10 * len(self.vocab))\n",
    "        return math.log(prob)\n",
    "\n",
    "# cài đặt beam_search thuật toán\n",
    "def beam_search_predict_accents(text_no_accents: str, model, k: int = 3, \n",
    "                                syllables_file: str = SYLLABLES_PATH, \n",
    "                                detokenizer=_detokenizer) -> list[tuple[str, float]]:\n",
    "    # Wrap dictionary model in CustomNGramModel if needed\n",
    "    if isinstance(model, dict) and \"n\" in model and \"vocab\" in model:\n",
    "        model = CustomNGramModel(model)\n",
    "    \n",
    "    words = text_no_accents.lower().split()\n",
    "    sequences = [] # Stores list of ([word_sequence], score)\n",
    "\n",
    "    for idx, word_no_accent in enumerate(words):\n",
    "        possible_accented_words = gen_accents_word(word_no_accent, syllables_path=syllables_file)\n",
    "        if not possible_accented_words:\n",
    "            possible_accented_words = {word_no_accent} \n",
    "\n",
    "        if idx == 0:\n",
    "            sequences = [([x], 0.0) for x in possible_accented_words]\n",
    "        else:\n",
    "            all_new_sequences = []\n",
    "            for seq_words, seq_score in sequences:\n",
    "                for next_accented_word in possible_accented_words:\n",
    "                    context = seq_words[-(model.order - 1):] if model.order > 1 else [] \n",
    "                    try:\n",
    "                        score_addition = model.logscore(next_accented_word, tuple(context))\n",
    "                    except Exception as e: \n",
    "                        # print(f\"Logscore error for '{next_accented_word}' with context {context}: {e}. Assigning low score.\")\n",
    "                        score_addition = -float('inf') \n",
    "                        \n",
    "                    new_seq_words = seq_words + [next_accented_word]\n",
    "                    all_new_sequences.append((new_seq_words, seq_score + score_addition))\n",
    "            \n",
    "            all_new_sequences = sorted(all_new_sequences, key=lambda x: x[1], reverse=True)\n",
    "            sequences = all_new_sequences[:k]\n",
    "            if not sequences: \n",
    "                if all_new_sequences:\n",
    "                    sequences = [(all_new_sequences[0][0][:-1] + [word_no_accent], all_new_sequences[0][1] - 1000)] \n",
    "                else:\n",
    "                    return []\n",
    "\n",
    "    results = [(detokenizer.detokenize(seq_words), score) for seq_words, score in sequences]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905ebc7",
   "metadata": {},
   "source": [
    "## 6. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2fc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porting evaluator code\n",
    "import json\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "RESULTS_FILE = \"evaluation_results.json\"\n",
    "TEST_SET_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "BEAM_K = 3\n",
    "\n",
    "# def _predict_item(item_data: tuple):\n",
    "#     unaccented_input_str, tokenized_true_accented_sent = item_data\n",
    "#     true_accented_str = _detokenizer.detokenize(tokenized_true_accented_sent)\n",
    "\n",
    "#     # Load model from the path\n",
    "#     model = load_model(DEFAULT_MODEL_PATH)\n",
    "#     if model is None:\n",
    "#         return {\n",
    "#             \"input_unaccented\": unaccented_input_str,\n",
    "#             \"true_accented\": true_accented_str,\n",
    "#             \"predicted_accented\": \"MODEL_LOAD_ERROR\"\n",
    "#         }\n",
    "\n",
    "#     predictions = beam_search_predict_accents(\n",
    "#         text_no_accents=unaccented_input_str,\n",
    "#         model=model,\n",
    "#         k=BEAM_K,\n",
    "#         syllables_file=SYLLABLES_PATH\n",
    "#     )\n",
    "\n",
    "#     predicted_accented_str = predictions[0][0] if predictions else \"\"\n",
    "\n",
    "#     return {\n",
    "#         \"input_unaccented\": unaccented_input_str,\n",
    "#         \"true_accented\": true_accented_str,\n",
    "#         \"predicted_accented\": predicted_accented_str\n",
    "#     }\n",
    "\n",
    "\n",
    "# Chỉnh sửa hàm _predict_item trong phần 6 (Evaluation Functions)\n",
    "def _predict_item(item_data: tuple):\n",
    "    unaccented_input_str, tokenized_true_accented_sent = item_data\n",
    "    true_accented_str = _detokenizer.detokenize(tokenized_true_accented_sent)\n",
    "\n",
    "    # Load model from the path\n",
    "    model = load_model(DEFAULT_MODEL_PATH)\n",
    "    if model is None:\n",
    "        return {\n",
    "            \"input_unaccented\": unaccented_input_str,\n",
    "            \"true_accented\": true_accented_str,\n",
    "            \"predicted_accented\": \"MODEL_LOAD_ERROR\"\n",
    "        }\n",
    "\n",
    "    # Thêm đoạn code này để chuyển đổi từ điển sang CustomNGramModel\n",
    "    if isinstance(model, dict) and \"n\" in model and \"vocab\" in model:\n",
    "        model = CustomNGramModel(model)\n",
    "\n",
    "    predictions = beam_search_predict_accents(\n",
    "        text_no_accents=unaccented_input_str,\n",
    "        model=model,\n",
    "        k=BEAM_K,\n",
    "        syllables_file=SYLLABLES_PATH\n",
    "    )\n",
    "\n",
    "    predicted_accented_str = predictions[0][0] if predictions else \"\"\n",
    "\n",
    "    return {\n",
    "        \"input_unaccented\": unaccented_input_str,\n",
    "        \"true_accented\": true_accented_str,\n",
    "        \"predicted_accented\": predicted_accented_str\n",
    "    }\n",
    "\n",
    "def calculate_sentence_accuracy(results: list[dict]) -> float:\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    correct_sentences = 0\n",
    "    for res in results:\n",
    "        if res[\"true_accented\"].strip() == res[\"predicted_accented\"].strip():\n",
    "            correct_sentences += 1\n",
    "    return (correct_sentences / len(results)) * 100\n",
    "\n",
    "def calculate_word_accuracy(results: list[dict]) -> float:\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    for res in results:\n",
    "        true_words = res[\"true_accented\"].strip().split()\n",
    "        predicted_words = res[\"predicted_accented\"].strip().split()\n",
    "        len_min = min(len(true_words), len(predicted_words))\n",
    "        for i in range(len_min):\n",
    "            if true_words[i] == predicted_words[i]:\n",
    "                correct_words += 1\n",
    "        total_words += len(true_words)\n",
    "    if total_words == 0: return 0.0\n",
    "    return (correct_words / total_words) * 100\n",
    "\n",
    "def calculate_cer(results: list[dict]) -> float:\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    total_edit_distance = 0\n",
    "    total_true_chars = 0\n",
    "    for res in results:\n",
    "        true_str = res[\"true_accented\"].strip()\n",
    "        pred_str = res[\"predicted_accented\"].strip()\n",
    "        if not true_str and not pred_str:\n",
    "            dist = 0\n",
    "        elif not true_str:\n",
    "            dist = len(pred_str)\n",
    "        elif not pred_str:\n",
    "            dist = len(true_str)\n",
    "        else:\n",
    "            dist = edit_distance(pred_str, true_str)\n",
    "        total_edit_distance += dist\n",
    "        total_true_chars += len(true_str)\n",
    "    if total_true_chars == 0: return 1.0 if total_edit_distance > 0 else 0.0\n",
    "    return (total_edit_distance / total_true_chars) * 100\n",
    "\n",
    "def display_sample_results(results: list[dict], num_samples: int = 5):\n",
    "    print(\"\\n--- Sample Predictions ---\")\n",
    "    for i, res in enumerate(results[:num_samples]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Input:     '{res['input_unaccented']}'\")\n",
    "        print(f\"  True:      '{res['true_accented']}'\")\n",
    "        print(f\"  Predicted: '{res['predicted_accented']}'\")\n",
    "        print(\"---\")\n",
    "\n",
    "def plot_metrics(metrics: dict):\n",
    "    \"\"\"Plots metrics using matplotlib.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        names = list(metrics.keys())\n",
    "        values = list(metrics.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(names, values)\n",
    "        plt.ylabel('Percentage (%)')\n",
    "        plt.title('Model Evaluation Metrics')\n",
    "\n",
    "        # Thêm giá trị trên mỗi cột\n",
    "        for i, value in enumerate(values):\n",
    "            plt.text(i, value + 0.5, f\"{value:.2f}%\", ha = 'center')\n",
    "\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, \"evaluation_metrics.png\"))\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"\\nMatplotlib not found. Skipping plot generation. Please install it: pip install matplotlib\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError plotting metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24bb03",
   "metadata": {},
   "source": [
    "## 7. Data Preparation and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01716286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing the Vietnamese language data...\n",
      "Downloading Vietnamese syllables file to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\vn_syllables.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading syllables file: 100%|██████████| 114k/114k [00:00<00:00, 1.45MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training data to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading training data: 100%|██████████| 64.5M/64.5M [00:14<00:00, 4.74MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full...\n",
      "Data directory configured at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\n",
      "Training data found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n",
      "Vietnamese syllables file found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\vn_syllables.txt\n",
      "\n",
      "Verifying data availability...\n",
      "Data directory configured at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\n",
      "Training data found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n",
      "Vietnamese syllables file found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\vn_syllables.txt\n",
      "Data directory configured at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\n",
      "Training data found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n",
      "Vietnamese syllables file found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\vn_syllables.txt\n",
      "\n",
      "Verifying data availability...\n",
      "Data directory configured at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\n",
      "Training data found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n",
      "Vietnamese syllables file found at: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\vn_syllables.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, download and prepare the data\n",
    "print(\"Downloading and preparing the Vietnamese language data...\")\n",
    "download_and_prepare_data()\n",
    "\n",
    "# Check if data exists\n",
    "print(\"\\nVerifying data availability...\")\n",
    "check_data_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be3a54",
   "metadata": {},
   "source": [
    "## 8. Train the N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10a1acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus for training...\n",
      "Loading corpus from: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files in Train_Full: 0it [00:00, ?it/s]\n",
      "Reading files in Chinh tri Xa hoi:   0%|          | 0/6567 [00:00<?, ?it/s]\n",
      "Reading files in Chinh tri Xa hoi: 100%|██████████| 6567/6567 [00:00<00:00, 12812.86it/s]\n",
      "Reading files in Chinh tri Xa hoi: 100%|██████████| 6567/6567 [00:00<00:00, 12812.86it/s]\n",
      "Reading files in Doi song: 100%|██████████| 4195/4195 [00:00<00:00, 12691.70it/s]\n",
      "Reading files in Doi song: 100%|██████████| 4195/4195 [00:00<00:00, 12691.70it/s]\n",
      "Reading files in Kinh doanh: 100%|██████████| 4276/4276 [00:00<00:00, 13488.81it/s]\n",
      "Reading files in Phap luat:   0%|          | 0/6656 [00:00<?, ?it/s]\n",
      "Reading files in Phap luat: 100%|██████████| 6656/6656 [00:00<00:00, 14235.83it/s]\n",
      "Reading files in Phap luat: 100%|██████████| 6656/6656 [00:00<00:00, 14235.83it/s]\n",
      "Reading files in Suc khoe: 100%|██████████| 4417/4417 [00:00<00:00, 13996.20it/s]\n",
      "Reading files in The gioi:   0%|          | 0/5716 [00:00<?, ?it/s]\n",
      "Reading files in The gioi: 100%|██████████| 5716/5716 [00:00<00:00, 13907.49it/s]\n",
      "Reading files in The gioi: 100%|██████████| 5716/5716 [00:00<00:00, 13907.49it/s]\n",
      "Reading files in The thao: 100%|██████████| 5667/5667 [00:00<00:00, 13007.69it/s]\n",
      "Reading files in The thao: 100%|██████████| 5667/5667 [00:00<00:00, 13007.69it/s]\n",
      "Reading files in Van hoa: 100%|██████████| 5250/5250 [00:00<00:00, 14227.76it/s]\n",
      "Reading files in Van hoa: 100%|██████████| 5250/5250 [00:00<00:00, 14227.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42744 documents.\n",
      "Processing 1208363 raw sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences: 100%|██████████| 1208363/1208363 [02:30<00:00, 8035.38it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created with 1001625 tokenized sentences.\n",
      "Corpus loaded with 1001625 sentences.\n",
      "\n",
      "Training the N-gram model...\n",
      "Preparing data for custom 3-gram model...\n",
      "Custom model training complete. Vocabulary size: 104810\n",
      "Number of unique 3-grams: 8052391\n",
      "Number of unique contexts: 1973514\n",
      "\n",
      "Saving the trained model...\n",
      "Saving model to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl...\n",
      "Custom model training complete. Vocabulary size: 104810\n",
      "Number of unique 3-grams: 8052391\n",
      "Number of unique contexts: 1973514\n",
      "\n",
      "Saving the trained model...\n",
      "Saving model to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl...\n",
      "Model successfully saved to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl\n",
      "Model successfully saved to c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the corpus\n",
    "print(\"Loading corpus for training...\")\n",
    "corpus = load_corpus(data_extract_path=TRAIN_EXTRACT_PATH)\n",
    "\n",
    "if corpus:\n",
    "    print(f\"Corpus loaded with {len(corpus)} sentences.\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nTraining the N-gram model...\")\n",
    "    trained_model = train_ngram_model(corpus, n=N_GRAM_ORDER)\n",
    "    \n",
    "    if trained_model:\n",
    "        # Save the model\n",
    "        print(\"\\nSaving the trained model...\")\n",
    "        save_model(trained_model, model_dir=MODEL_DIR, filename=DEFAULT_MODEL_FILENAME)\n",
    "    else:\n",
    "        print(\"Model training failed.\")\n",
    "else:\n",
    "    print(\"Corpus could not be loaded. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5517a8f",
   "metadata": {},
   "source": [
    "## 9. Test the Accent Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93165e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained model for prediction...\n",
      "Attempting to load model from c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl...\n",
      "Model loaded successfully from c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl. Vocabulary size: 104810\n",
      "\n",
      "Testing accent prediction on sample sentences:\n",
      "\n",
      "Sample 1: 'ngay hom qua la ngay bau cu tong thong my'\n",
      "Model loaded successfully from c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\models\\kneserney_trigram_model.pkl. Vocabulary size: 104810\n",
      "\n",
      "Testing accent prediction on sample sentences:\n",
      "\n",
      "Sample 1: 'ngay hom qua la ngay bau cu tong thong my'\n",
      "Top predictions:\n",
      "1. 'ngày hôm qua là ngày bẩu cự tộng thống mý' (Score: -134.2986)\n",
      "2. 'ngày hôm qua là ngày bẩu cự tộng thống mỹ' (Score: -134.2986)\n",
      "3. 'ngày hôm qua là ngày bẩu cự tộng thống my' (Score: -134.2986)\n",
      "\n",
      "Sample 2: 'chuc mung nam moi'\n",
      "Top predictions:\n",
      "1. 'ngày hôm qua là ngày bẩu cự tộng thống mý' (Score: -134.2986)\n",
      "2. 'ngày hôm qua là ngày bẩu cự tộng thống mỹ' (Score: -134.2986)\n",
      "3. 'ngày hôm qua là ngày bẩu cự tộng thống my' (Score: -134.2986)\n",
      "\n",
      "Sample 2: 'chuc mung nam moi'\n",
      "Top predictions:\n",
      "1. 'chúc mừng năm mới' (Score: -5.4654)\n",
      "2. 'chúc mừng năm mối' (Score: -32.8219)\n",
      "3. 'chúc mừng năm mội' (Score: -32.8219)\n",
      "\n",
      "Sample 3: 'toi yeu tieng viet'\n",
      "Top predictions:\n",
      "1. 'chúc mừng năm mới' (Score: -5.4654)\n",
      "2. 'chúc mừng năm mối' (Score: -32.8219)\n",
      "3. 'chúc mừng năm mội' (Score: -32.8219)\n",
      "\n",
      "Sample 3: 'toi yeu tieng viet'\n",
      "Top predictions:\n",
      "1. 'tôi yêu tiếng việt' (Score: -35.3680)\n",
      "2. 'tôi yêu tiếng viet' (Score: -35.3680)\n",
      "3. 'tôi yêu tiếng viết' (Score: -35.3680)\n",
      "\n",
      "Sample 4: 'hoc sinh truong thpt'\n",
      "Top predictions:\n",
      "1. 'tôi yêu tiếng việt' (Score: -35.3680)\n",
      "2. 'tôi yêu tiếng viet' (Score: -35.3680)\n",
      "3. 'tôi yêu tiếng viết' (Score: -35.3680)\n",
      "\n",
      "Sample 4: 'hoc sinh truong thpt'\n",
      "Top predictions:\n",
      "1. 'học sinh trường thpt' (Score: -7.2259)\n",
      "2. 'hoc sinh truong thpt' (Score: -49.3098)\n",
      "3. 'học sinh truống thpt' (Score: -56.0576)\n",
      "\n",
      "Sample 5: 'cau lac bo am nhac'\n",
      "Top predictions:\n",
      "1. 'học sinh trường thpt' (Score: -7.2259)\n",
      "2. 'hoc sinh truong thpt' (Score: -49.3098)\n",
      "3. 'học sinh truống thpt' (Score: -56.0576)\n",
      "\n",
      "Sample 5: 'cau lac bo am nhac'\n",
      "Top predictions:\n",
      "1. 'câu lạc bộ ẩm nhắc' (Score: -32.4795)\n",
      "2. 'câu lạc bộ ẩm nhác' (Score: -32.4795)\n",
      "3. 'câu lạc bộ ẩm nhạc' (Score: -32.4795)\n",
      "Top predictions:\n",
      "1. 'câu lạc bộ ẩm nhắc' (Score: -32.4795)\n",
      "2. 'câu lạc bộ ẩm nhác' (Score: -32.4795)\n",
      "3. 'câu lạc bộ ẩm nhạc' (Score: -32.4795)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "print(\"Loading the trained model for prediction...\")\n",
    "model = load_model(DEFAULT_MODEL_PATH)\n",
    "\n",
    "if model:\n",
    "    # Test some sample sentences\n",
    "    test_sentences = [\n",
    "        \"ngay hom qua la ngay bau cu tong thong my\",\n",
    "        \"chuc mung nam moi\",\n",
    "        \"toi yeu tieng viet\",\n",
    "        \"hoc sinh truong thpt\",\n",
    "        \"cau lac bo am nhac\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting accent prediction on sample sentences:\")\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        print(f\"\\nSample {i+1}: '{sentence}'\")\n",
    "        predictions = beam_search_predict_accents(sentence, model, k=3, syllables_file=SYLLABLES_PATH)\n",
    "        \n",
    "        if predictions:\n",
    "            print(\"Top predictions:\")\n",
    "            for j, (sent, score) in enumerate(predictions):\n",
    "                print(f\"{j+1}. '{sent}' (Score: {score:.4f})\")\n",
    "        else:\n",
    "            print(\"No predictions returned.\")\n",
    "else:\n",
    "    print(\"Model could not be loaded. Cannot run predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325b2f4",
   "metadata": {},
   "source": [
    "## 10. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for model evaluation...\n",
      "\n",
      "Loading and splitting corpus for evaluation...\n",
      "Loading corpus from: c:\\Users\\tontide1\\Desktop\\Vietnamese-Accent-Model\\data\\Train_Full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files in Train_Full: 0it [00:00, ?it/s]\n",
      "Reading files in Chinh tri Xa hoi:   0%|          | 0/6567 [00:00<?, ?it/s]\n",
      "Reading files in Chinh tri Xa hoi: 100%|██████████| 6567/6567 [00:00<00:00, 14496.13it/s]\n",
      "Reading files in Chinh tri Xa hoi: 100%|██████████| 6567/6567 [00:00<00:00, 14496.13it/s]\n",
      "Reading files in Doi song: 100%|██████████| 4195/4195 [00:00<00:00, 13551.81it/s]\n",
      "Reading files in Doi song: 100%|██████████| 4195/4195 [00:00<00:00, 13551.81it/s]\n",
      "Reading files in Kinh doanh: 100%|██████████| 4276/4276 [00:00<00:00, 13391.61it/s]\n",
      "Reading files in Kinh doanh: 100%|██████████| 4276/4276 [00:00<00:00, 13391.61it/s]\n",
      "Reading files in Phap luat: 100%|██████████| 6656/6656 [00:00<00:00, 14706.79it/s]\n",
      "Reading files in Phap luat: 100%|██████████| 6656/6656 [00:00<00:00, 14706.79it/s]\n",
      "Reading files in Suc khoe: 100%|██████████| 4417/4417 [00:00<00:00, 14434.82it/s]\n",
      "Reading files in Suc khoe: 100%|██████████| 4417/4417 [00:00<00:00, 14434.82it/s]\n",
      "Reading files in The gioi: 100%|██████████| 5716/5716 [00:00<00:00, 14903.06it/s]\n",
      "Reading files in The gioi: 100%|██████████| 5716/5716 [00:00<00:00, 14903.06it/s]\n",
      "Reading files in The thao: 100%|██████████| 5667/5667 [00:00<00:00, 13870.33it/s]\n",
      "Reading files in The thao: 100%|██████████| 5667/5667 [00:00<00:00, 13870.33it/s]\n",
      "Reading files in Van hoa: 100%|██████████| 5250/5250 [00:00<00:00, 14623.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42744 documents.\n",
      "CẢNH BÁO: Giới hạn xử lý 100 câu đầu tiên trên tổng số 1208363 câu để tiết kiệm bộ nhớ.\n",
      "Processing 100 raw sentences for splitting using multiprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating unaccented and tokenizing (parallel): 100%|██████████| 100/100 [00:00<00:00, 131.27it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing for model evaluation...\")\n",
    "\n",
    "# Prepare evaluation data\n",
    "print(\"\\nLoading and splitting corpus for evaluation...\")\n",
    "_, test_set = load_and_split_corpus(data_extract_path=TRAIN_EXTRACT_PATH, test_size=TEST_SET_SIZE, random_seed=RANDOM_SEED)\n",
    "\n",
    "if test_set:\n",
    "    print(f\"Loaded {len(test_set)} test items for evaluation.\")\n",
    "    \n",
    "    # Limit test set size for faster evaluation\n",
    "    MAX_EVAL_ITEMS = 50\n",
    "    if len(test_set) > MAX_EVAL_ITEMS:\n",
    "        print(f\"Limiting evaluation to first {MAX_EVAL_ITEMS} items to save time...\")\n",
    "        test_set = test_set[:MAX_EVAL_ITEMS]\n",
    "    \n",
    "    # Evaluate each test item\n",
    "    print(\"\\nRunning predictions on test items...\")\n",
    "    evaluation_results = []\n",
    "    \n",
    "    from tqdm.notebook import tqdm\n",
    "    for item in tqdm(test_set, desc=\"Evaluating\"):\n",
    "        result = _predict_item(item)\n",
    "        evaluation_results.append(result)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating evaluation metrics...\")\n",
    "    sent_accuracy = calculate_sentence_accuracy(evaluation_results)\n",
    "    word_accuracy = calculate_word_accuracy(evaluation_results)\n",
    "    char_error_rate = calculate_cer(evaluation_results)\n",
    "    char_accuracy = 100.0 - char_error_rate\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"\\n--- Evaluation Metrics ---\")\n",
    "    print(f\"Sentence Accuracy: {sent_accuracy:.2f}%\")\n",
    "    print(f\"Word Accuracy: {word_accuracy:.2f}%\")\n",
    "    print(f\"Character Error Rate (CER): {char_error_rate:.2f}%\")\n",
    "    print(f\"Character Accuracy: {char_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    results_file_path = os.path.join(BASE_DIR, RESULTS_FILE)\n",
    "    try:\n",
    "        with open(results_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(evaluation_results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\nResults saved to {results_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    # Display sample results\n",
    "    display_sample_results(evaluation_results, num_samples=5)\n",
    "    \n",
    "    # Plot metrics\n",
    "    metrics_to_plot = {\n",
    "        \"Sentence Accuracy\": sent_accuracy,\n",
    "        \"Word Accuracy\": word_accuracy,\n",
    "        \"Character Accuracy\": char_accuracy\n",
    "    }\n",
    "    plot_metrics(metrics_to_plot)\n",
    "else:\n",
    "    print(\"Test set could not be loaded. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5213a",
   "metadata": {},
   "source": [
    "## 11. Interactive Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_demo():\n",
    "    # Load the model\n",
    "    model = load_model(DEFAULT_MODEL_PATH)\n",
    "    if not model:\n",
    "        print(\"Model could not be loaded. Cannot run prediction demo.\")\n",
    "        return\n",
    "    \n",
    "    print(\"--- Vietnamese Accent Predictor Demo ---\")\n",
    "    print(\"Enter Vietnamese text without accents to predict the accented version.\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        text_input = input(\"\\nNhập câu tiếng Việt không dấu: \")\n",
    "        if text_input.lower() == 'exit':\n",
    "            break\n",
    "        if not text_input.strip():\n",
    "            print(\"Please enter a sentence.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Predicting accents for: '{text_input}'\")\n",
    "        predictions = beam_search_predict_accents(\n",
    "            text_input, \n",
    "            model, \n",
    "            k=3, \n",
    "            syllables_file=SYLLABLES_PATH\n",
    "        )\n",
    "        \n",
    "        if predictions:\n",
    "            print(\"\\nTop predictions:\")\n",
    "            for i, (sent, score) in enumerate(predictions):\n",
    "                print(f\"{i+1}. '{sent}' (Score: {score:.4f})\")\n",
    "        else:\n",
    "            print(\"No predictions returned.\")\n",
    "    \n",
    "    print(\"\\n--- Demo Finished ---\")\n",
    "\n",
    "# Uncomment to run the interactive demo\n",
    "# run_prediction_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b7872",
   "metadata": {},
   "source": [
    "## 12. Implement the Interactive Widget (Google Colab specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import display\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    # Load the model outside the function to avoid reloading it for each prediction\n",
    "    print(\"Loading model for interactive widget...\")\n",
    "    interactive_model = load_model(DEFAULT_MODEL_PATH)\n",
    "    \n",
    "    if interactive_model:\n",
    "        print(\"Model loaded successfully.\")\n",
    "        \n",
    "        def predict_button_clicked(b):\n",
    "            input_text = input_widget.value\n",
    "            if not input_text.strip():\n",
    "                output_widget.value = \"Please enter a sentence.\"\n",
    "                return\n",
    "                \n",
    "            output_widget.value = f\"Predicting accents for: '{input_text}'\\n\\n\"\n",
    "            predictions = beam_search_predict_accents(\n",
    "                input_text, \n",
    "                interactive_model, \n",
    "                k=3, \n",
    "                syllables_file=SYLLABLES_PATH\n",
    "            )\n",
    "            \n",
    "            if predictions:\n",
    "                output_widget.value += \"Top predictions:\\n\"\n",
    "                for i, (sent, score) in enumerate(predictions):\n",
    "                    output_widget.value += f\"{i+1}. '{sent}' (Score: {score:.4f})\\n\"\n",
    "            else:\n",
    "                output_widget.value += \"No predictions returned.\"\n",
    "        \n",
    "        # Create widgets\n",
    "        input_widget = widgets.Text(\n",
    "            description='Input:',\n",
    "            placeholder='Enter Vietnamese text without accents',\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "        \n",
    "        predict_button = widgets.Button(\n",
    "            description='Predict Accents',\n",
    "            button_style='primary',\n",
    "            tooltip='Click to predict accents'\n",
    "        )\n",
    "        predict_button.on_click(predict_button_clicked)\n",
    "        \n",
    "        output_widget = widgets.Textarea(\n",
    "            placeholder='Predictions will appear here',\n",
    "            layout=widgets.Layout(width='80%', height='200px')\n",
    "        )\n",
    "        \n",
    "        # Display the interactive interface\n",
    "        print(\"\\n--- Vietnamese Accent Predictor ---\")\n",
    "        display(widgets.VBox([input_widget, predict_button, output_widget]))\n",
    "    else:\n",
    "        print(\"Model could not be loaded. Cannot initialize interactive widget.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Could not import ipywidgets. Running in a non-interactive environment or ipywidgets not installed.\")\n",
    "    print(\"To use the interactive widget, please install ipywidgets: !pip install ipywidgets\")\n",
    "    print(\"You can still use the text-based demo by uncommenting and running the call to run_prediction_demo().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb21ba2",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Additional Resources\n",
    "\n",
    "This notebook contains the complete Vietnamese Accent Model code, adapted to run on Google Colab. The model is trained to predict appropriate Vietnamese accents for text without diacritics.\n",
    "\n",
    "### Summary of Components:\n",
    "\n",
    "1. **Utilities**: Basic text processing functions for tokenization and accent manipulation\n",
    "2. **Data Loader**: Functions to download, prepare, and load the Vietnamese text corpus\n",
    "3. **Model Trainer**: N-gram model training implementation \n",
    "4. **Predictor**: Beam search algorithm for predicting accents in Vietnamese text\n",
    "5. **Evaluation**: Metrics calculation and visualization for model performance\n",
    "6. **Interactive Demo**: User interface for testing accent prediction\n",
    "\n",
    "### Further Improvements:\n",
    "\n",
    "- Implement more sophisticated smoothing techniques for the language model\n",
    "- Incorporate character-level features for better accent prediction\n",
    "- Optimize the beam search algorithm for better performance\n",
    "- Experiment with neural models for accent prediction\n",
    "\n",
    "Feel free to modify and extend this code for your own research or applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
