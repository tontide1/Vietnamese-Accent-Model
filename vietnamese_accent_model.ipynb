{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1c39ba",
   "metadata": {},
   "source": [
    "# Vietnamese Accent Prediction Model\n",
    "\n",
    "This notebook contains the complete implementation of a Vietnamese accent prediction model. It allows you to train, evaluate, and use a model that predicts accents in Vietnamese text.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Vietnamese text without accents (diacritics) is ambiguous as many words can have different meanings depending on the tone marks. This model uses n-gram language modeling to predict the most likely accented version of Vietnamese text.\n",
    "\n",
    "**Key Components:**\n",
    "* Utility functions for text processing\n",
    "* Data loading and corpus preparation\n",
    "* N-gram model training\n",
    "* Accent prediction using beam search\n",
    "* Model evaluation metrics\n",
    "* Interactive prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba007e",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "This section installs all required dependencies and sets up the environment for the Vietnamese Accent Prediction model to work in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install nltk==3.5 scikit-learn==1.2.2 dill~=0.3.7 pandas~=2.0.3 matplotlib~=3.7.5 seaborn~=0.13.2 tqdm~=4.66.0 requests~=2.31.0\n",
    "\n",
    "# Import common libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.lm import KneserNeyInterpolated, Laplace, WittenBellInterpolated\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create necessary directories\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "TRAIN_EXTRACT_PATH = os.path.join(DATA_DIR, \"Train_Full\")\n",
    "SYLLABLES_NAME = \"vn_syllables.txt\"\n",
    "SYLLABLES_PATH = os.path.join(DATA_DIR, SYLLABLES_NAME)\n",
    "DEFAULT_MODEL_FILENAME = \"kneserney_trigram_model.pkl\"\n",
    "N_GRAM_ORDER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f4f67",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "\n",
    "This section defines core utility functions for text processing, including tokenization and accent removal. These functions form the foundation of the model's ability to process Vietnamese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ae645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for Vietnamese text processing (from utils.py)\n",
    "\n",
    "def tokenize(doc: str) -> list[str]:\n",
    "    \"\"\"Tokenize a document into words.\"\"\"\n",
    "    tokens = nltk.word_tokenize(doc.lower())\n",
    "    # Allow underscore, remove other punctuation\n",
    "    table = str.maketrans('', '', string.punctuation.replace(\"_\", \"\"))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word]  # Remove empty strings\n",
    "    return tokens\n",
    "\n",
    "def remove_vn_accent(word: str) -> str:\n",
    "    \"\"\"Remove Vietnamese accents from a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[áàảãạăắằẳẵặâấầẩẫậ]', 'a', word)\n",
    "    word = re.sub(r'[éèẻẽẹêếềểễệ]', 'e', word)\n",
    "    word = re.sub(r'[óòỏõọôốồổỗộơớờởỡợ]', 'o', word)\n",
    "    word = re.sub(r'[íìỉĩị]', 'i', word)\n",
    "    word = re.sub(r'[úùủũụưứừửữự]', 'u', word)\n",
    "    word = re.sub(r'[ýỳỷỹỵ]', 'y', word)\n",
    "    word = re.sub(r'đ', 'd', word)\n",
    "    return word\n",
    "\n",
    "def gen_accents_word(word: str, syllables_path: str = SYLLABLES_PATH) -> set[str]:\n",
    "    \"\"\"Generate all possible accented forms of a word using a syllables file.\"\"\"\n",
    "    normalized_input_word = word.lower()\n",
    "    word_no_accent = remove_vn_accent(normalized_input_word)\n",
    "    all_accent_word = {normalized_input_word}  # Start with the input word (normalized)\n",
    "\n",
    "    if not os.path.exists(syllables_path):\n",
    "        print(f\"Warning: Syllables file not found at {syllables_path}. \"\n",
    "              f\"Accent generation will be limited to the input word: '{word}'.\")\n",
    "        if word_no_accent != normalized_input_word:\n",
    "            all_accent_word.add(word_no_accent)\n",
    "        return all_accent_word\n",
    "\n",
    "    try:\n",
    "        with open(syllables_path, 'r', encoding='utf-8') as f:\n",
    "            for w_line in f.read().splitlines():\n",
    "                w_line_lower = w_line.lower()  # Normalize file content\n",
    "                if remove_vn_accent(w_line_lower) == word_no_accent:\n",
    "                    all_accent_word.add(w_line_lower)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing syllables file {syllables_path}: {e}\")\n",
    "    \n",
    "    return all_accent_word\n",
    "\n",
    "# Test the utility functions\n",
    "print(\"Tokenize example:\")\n",
    "print(tokenize(\"Đây_là một câu, ví dụ.\"))\n",
    "\n",
    "print(\"\\nRemove accent example:\")\n",
    "print(remove_vn_accent(\"hoàng\"))\n",
    "print(remove_vn_accent(\"Hoàng\"))\n",
    "\n",
    "# Create a sample syllables file for testing if it doesn't exist\n",
    "if not os.path.exists(SYLLABLES_PATH):\n",
    "    print(f\"\\nCreating sample syllables file at {SYLLABLES_PATH} for testing...\")\n",
    "    try:\n",
    "        with open(SYLLABLES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"hoàng\\nhoang\\nhọang\\nhởang\\nhờang\\nhoa\\nhòa\\nviệt\\n\") # Add some test data\n",
    "        print(f\"Created sample syllables file with test data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating sample syllables file: {e}\")\n",
    "\n",
    "print(\"\\nGenerate accents example:\")\n",
    "print(f\"'hoang': {gen_accents_word('hoang')}\")\n",
    "print(f\"'hoa': {gen_accents_word('hoa')}\")\n",
    "print(f\"'viet': {gen_accents_word('viet')}\")\n",
    "print(f\"'HoÀnG': {gen_accents_word('HoÀnG')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab6005",
   "metadata": {},
   "source": [
    "## 3. Data Loader Module\n",
    "\n",
    "This section implements the data loading functionality from `data_loader.py`. It includes functions for checking data existence, loading the corpus, and splitting data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader functions (from data_loader.py)\n",
    "\n",
    "# Constants for downloading data if needed\n",
    "TRAIN_ZIP_URL = \"https://github.com/hoanganhpham1006/Vietnamese_Language_Model/raw/master/Train_Full.zip\"\n",
    "TRAIN_ZIP_NAME = \"Train_Full.zip\"\n",
    "TRAIN_ZIP_PATH = os.path.join(DATA_DIR, TRAIN_ZIP_NAME)\n",
    "\n",
    "SYLLABLES_URL = \"https://gist.githubusercontent.com/hieuthi/0f5adb7d3f79e7fb67e0e499004bf558/raw/135a4d9716e49a981624474156d6f247b9b46f6a/all-vietnamese-syllables.txt\"\n",
    "\n",
    "def download_and_prepare_data():\n",
    "    \"\"\"Download and extract training data and syllables file.\"\"\"\n",
    "    import requests\n",
    "    import zipfile\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # Download training data zip file if it doesn't exist\n",
    "    if not os.path.exists(TRAIN_EXTRACT_PATH) or not os.listdir(TRAIN_EXTRACT_PATH):\n",
    "        print(f\"Downloading training data from {TRAIN_ZIP_URL}...\")\n",
    "        if not os.path.exists(TRAIN_ZIP_PATH):\n",
    "            try:\n",
    "                response = requests.get(TRAIN_ZIP_URL, stream=True)\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                block_size = 8192\n",
    "                with open(TRAIN_ZIP_PATH, 'wb') as f, tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                    for data in response.iter_content(block_size):\n",
    "                        f.write(data)\n",
    "                        pbar.update(len(data))\n",
    "                print(f\"Downloaded training data to {TRAIN_ZIP_PATH}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading training data: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Extract training data zip file\n",
    "        print(f\"Extracting training data to {TRAIN_EXTRACT_PATH}...\")\n",
    "        try:\n",
    "            os.makedirs(TRAIN_EXTRACT_PATH, exist_ok=True)\n",
    "            with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
    "                zip_ref.extractall(DATA_DIR)\n",
    "            print(f\"Extracted training data to {TRAIN_EXTRACT_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting training data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Download syllables file if it doesn't exist\n",
    "    if not os.path.exists(SYLLABLES_PATH):\n",
    "        print(f\"Downloading Vietnamese syllables from {SYLLABLES_URL}...\")\n",
    "        try:\n",
    "            response = requests.get(SYLLABLES_URL)\n",
    "            with open(SYLLABLES_PATH, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded Vietnamese syllables to {SYLLABLES_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading Vietnamese syllables: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_data_exists():\n",
    "    \"\"\"Check if required data exists.\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Data directory configured at: {DATA_DIR}\")\n",
    "    all_data_exists = True\n",
    "    if not os.path.exists(TRAIN_EXTRACT_PATH) or not os.listdir(TRAIN_EXTRACT_PATH):\n",
    "        print(f\"Warning: Training data directory {TRAIN_EXTRACT_PATH} is missing or empty.\")\n",
    "        print(\"Please ensure you have downloaded and extracted 'Train_Full.zip' into the 'data/Train_Full' directory.\")\n",
    "        all_data_exists = False\n",
    "    else:\n",
    "        print(f\"Training data found at: {TRAIN_EXTRACT_PATH}\")\n",
    "\n",
    "    if not os.path.exists(SYLLABLES_PATH):\n",
    "        print(f\"Warning: Vietnamese syllables file {SYLLABLES_PATH} is missing.\")\n",
    "        print(\"Please ensure you have 'vn_syllables.txt' in the 'data' directory.\")\n",
    "        all_data_exists = False\n",
    "    else:\n",
    "        print(f\"Vietnamese syllables file found at: {SYLLABLES_PATH}\")\n",
    "    return all_data_exists\n",
    "\n",
    "def load_corpus(data_extract_path: str = TRAIN_EXTRACT_PATH) -> list[list[str]]:\n",
    "    \"\"\"Load corpus from extracted training data.\"\"\"\n",
    "    if not os.path.exists(data_extract_path):\n",
    "        print(f\"Error: Training data path not found: {data_extract_path}\")\n",
    "        print(\"Please run download_and_prepare_data() first or ensure data is correctly placed.\")\n",
    "        return []\n",
    "\n",
    "    full_text_content = []\n",
    "    print(f\"Loading corpus from: {data_extract_path}\")\n",
    "\n",
    "    for dirname, _, filenames in os.walk(data_extract_path):\n",
    "        for filename in tqdm(filenames, desc=f\"Reading files in {os.path.basename(dirname)}\"):\n",
    "            if filename.endswith(\".txt\"): \n",
    "                try:\n",
    "                    with open(os.path.join(dirname, filename), 'r', encoding='UTF-16') as f:\n",
    "                        full_text_content.append(f.read())\n",
    "                except Exception as e:\n",
    "                    # Try with different encodings if UTF-16 fails\n",
    "                    try:\n",
    "                        with open(os.path.join(dirname, filename), 'r', encoding='utf-8') as f:\n",
    "                            full_text_content.append(f.read())\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Could not read file {os.path.join(dirname, filename)}: {e2}\")\n",
    "    \n",
    "    if not full_text_content:\n",
    "        print(\"No text files found or loaded from the training data path.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Loaded {len(full_text_content)} documents.\")\n",
    "    full_data_string = \". \".join(full_text_content)\n",
    "    full_data_string = full_data_string.replace(\"\\n\", \". \")\n",
    "    \n",
    "    corpus = []\n",
    "    raw_sents = re.split(r'[.?!]\\s+', full_data_string) \n",
    "    print(f\"Processing {len(raw_sents)} raw sentences...\")\n",
    "    for sent in tqdm(raw_sents, desc=\"Tokenizing sentences\"):\n",
    "        if sent.strip(): # Ensure sentence is not just whitespace\n",
    "            corpus.append(tokenize(sent))\n",
    "    \n",
    "    print(f\"Corpus created with {len(corpus)} tokenized sentences.\")\n",
    "    return corpus\n",
    "\n",
    "def _process_single_sentence_for_splitting(sent_accented: str):\n",
    "    \"\"\"Process a single sentence for parallel corpus splitting.\"\"\"\n",
    "    if sent_accented.strip():\n",
    "        temp_tokenized_for_unaccenting = tokenize(sent_accented)\n",
    "        unaccented_words = [remove_vn_accent(word) for word in temp_tokenized_for_unaccenting]\n",
    "        unaccented_sentence_str = \" \".join(unaccented_words)\n",
    "        tokenized_accented_sentence = tokenize(sent_accented)\n",
    "        if unaccented_sentence_str and tokenized_accented_sentence:\n",
    "            return (unaccented_sentence_str, tokenized_accented_sentence)\n",
    "    return None\n",
    "\n",
    "def load_and_split_corpus(data_extract_path: str = TRAIN_EXTRACT_PATH, test_size: float = 0.2, random_seed: int = 42):\n",
    "    \"\"\"Load and split corpus into training and testing sets.\"\"\"\n",
    "    if not os.path.exists(data_extract_path):\n",
    "        print(f\"Error: Training data path not found: {data_extract_path}\")\n",
    "        return [], []\n",
    "\n",
    "    full_text_content = []\n",
    "    print(f\"Loading corpus from: {data_extract_path}\")\n",
    "    for dirname, _, filenames in os.walk(data_extract_path):\n",
    "        for filename in tqdm(filenames, desc=f\"Reading files in {os.path.basename(dirname)}\"):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                try:\n",
    "                    with open(os.path.join(dirname, filename), 'r', encoding='UTF-16') as f:\n",
    "                        full_text_content.append(f.read())\n",
    "                except Exception as e:\n",
    "                    # Try with different encodings if UTF-16 fails\n",
    "                    try:\n",
    "                        with open(os.path.join(dirname, filename), 'r', encoding='utf-8') as f:\n",
    "                            full_text_content.append(f.read())\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Could not read file {os.path.join(dirname, filename)}: {e2}\")\n",
    "    \n",
    "    if not full_text_content:\n",
    "        print(\"No text files found or loaded from the training data path.\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"Loaded {len(full_text_content)} documents.\")\n",
    "    full_data_string = \". \".join(full_text_content)\n",
    "    full_data_string = full_data_string.replace(\"\\n\", \". \")\n",
    "    \n",
    "    raw_sentences_with_accent = re.split(r'[.?!]\\s+', full_data_string)\n",
    "\n",
    "    # Limit number of sentences to avoid memory errors\n",
    "    MAX_PROCESS_SENTENCES = 1000\n",
    "    if len(raw_sentences_with_accent) > MAX_PROCESS_SENTENCES:\n",
    "        print(f\"WARNING: Limiting processing to {MAX_PROCESS_SENTENCES} sentences out of {len(raw_sentences_with_accent)} to save memory.\")\n",
    "        raw_sentences_with_accent = raw_sentences_with_accent[:MAX_PROCESS_SENTENCES]\n",
    "    \n",
    "    print(f\"Processing {len(raw_sentences_with_accent)} raw sentences for splitting...\")\n",
    "    \n",
    "    # Process sentences in parallel if possible\n",
    "    processed_sentences = []\n",
    "    if mp.cpu_count() > 1:\n",
    "        with mp.Pool() as pool:\n",
    "            num_cores = os.cpu_count() or 1\n",
    "            chunk_size = max(1, len(raw_sentences_with_accent) // (num_cores * 4))\n",
    "            \n",
    "            results_iterator = pool.imap(_process_single_sentence_for_splitting, \n",
    "                                        tqdm(raw_sentences_with_accent, desc=\"Generating unaccented and tokenizing (parallel)\"),\n",
    "                                        chunksize=chunk_size)\n",
    "            \n",
    "            # Filter out None results\n",
    "            processed_sentences = [res for res in results_iterator if res is not None]\n",
    "    else:\n",
    "        # Sequential processing if multiprocessing is not available\n",
    "        for sent in tqdm(raw_sentences_with_accent, desc=\"Generating unaccented and tokenizing\"):\n",
    "            result = _process_single_sentence_for_splitting(sent)\n",
    "            if result is not None:\n",
    "                processed_sentences.append(result)\n",
    "\n",
    "    if not processed_sentences:\n",
    "        print(\"No sentences could be processed. Check data and tokenization.\")\n",
    "        return [], []\n",
    "        \n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(processed_sentences)\n",
    "    \n",
    "    split_index = int(len(processed_sentences) * (1 - test_size))\n",
    "    train_data_pairs = processed_sentences[:split_index]\n",
    "    test_data_pairs = processed_sentences[split_index:]\n",
    "    \n",
    "    train_corpus = [pair[1] for pair in train_data_pairs] # Only take tokenized accented sentences for training\n",
    "    test_set = test_data_pairs # Keep full pairs (unaccented_string, tokenized_accented_sentence) for testing\n",
    "    \n",
    "    print(f\"Data split: {len(train_corpus)} training sentences, {len(test_set)} test sentences.\")\n",
    "    return train_corpus, test_set\n",
    "\n",
    "# Check if data exists and download if necessary\n",
    "print(\"Checking if required data exists...\")\n",
    "if not check_data_exists():\n",
    "    print(\"Required data not found. Attempting to download...\")\n",
    "    download_and_prepare_data()\n",
    "    if not check_data_exists():\n",
    "        print(\"Warning: Data preparation failed. You may need to create sample data for testing.\")\n",
    "        # Create minimal sample data for testing in Google Colab\n",
    "        os.makedirs(TRAIN_EXTRACT_PATH, exist_ok=True)\n",
    "        sample_text = \"Đây là một câu tiếng Việt có dấu để kiểm tra. Xin chào Việt Nam. Tiếng Việt rất hay.\"\n",
    "        with open(os.path.join(TRAIN_EXTRACT_PATH, \"sample.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(sample_text)\n",
    "        print(\"Created minimal sample data for testing.\")\n",
    "else:\n",
    "    print(\"Required data exists.\")\n",
    "\n",
    "# Load a small corpus for demonstration\n",
    "print(\"\\nLoading a small corpus for demonstration...\")\n",
    "train_corpus, test_set = load_and_split_corpus(test_size=0.2, random_seed=42)\n",
    "print(f\"Loaded {len(train_corpus)} training sentences and {len(test_set)} test pairs.\")\n",
    "\n",
    "# Show sample data\n",
    "if train_corpus:\n",
    "    print(\"\\nSample training sentences (tokenized):\\n\", train_corpus[:2])\n",
    "if test_set:\n",
    "    print(\"\\nSample test pairs (unaccented, tokenized_accented):\\n\", test_set[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1df12",
   "metadata": {},
   "source": [
    "## 4. Model Trainer Module\n",
    "\n",
    "This section implements the model training functionality from `model_trainer.py`. It includes functions for training n-gram models and saving trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae09c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trainer functions (from model_trainer.py)\n",
    "\n",
    "def train_ngram_model(corpus: list[list[str]], n: int = N_GRAM_ORDER):\n",
    "    \"\"\"Train an n-gram language model.\"\"\"\n",
    "    if not corpus:\n",
    "        print(\"Corpus is empty. Cannot train model.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing data for custom {n}-gram model...\")\n",
    "\n",
    "    start_symbol = \"<s>\"\n",
    "    end_symbol = \"</s>\"\n",
    "\n",
    "    ngram_counts = defaultdict(int)\n",
    "    context_counts = defaultdict(int) # For (n-1)-grams\n",
    "    vocab = set()\n",
    "\n",
    "    for sentence_tokens in corpus:\n",
    "        # Pad sentence: (n-1) start symbols, 1 end symbol.\n",
    "        # e.g., for n=3 (trigram): ['<s>', '<s>'] + sentence_tokens + ['</s>']\n",
    "        # e.g., for n=1 (unigram): [] + sentence_tokens + ['</s>']\n",
    "        current_padded_sentence = ([start_symbol] * (n - 1)) + sentence_tokens + [end_symbol]\n",
    "\n",
    "        for token in sentence_tokens: # Original sentence tokens for vocab\n",
    "            vocab.add(token)\n",
    "\n",
    "        # Generate n-grams and (n-1)-gram contexts\n",
    "        # Iterate up to the point where the last n-gram can be formed\n",
    "        for i in range(len(current_padded_sentence) - n + 1):\n",
    "            ngram = tuple(current_padded_sentence[i : i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "\n",
    "            if n > 1:\n",
    "                # The context is the first (n-1) tokens of the n-gram\n",
    "                context = tuple(current_padded_sentence[i : i + n - 1])\n",
    "                context_counts[context] += 1\n",
    "            # For n=1 (unigrams), context_counts will be handled later if needed for P(w) = count(w)/TotalWords\n",
    "\n",
    "    # For unigram model (n=1), if we define P(w) = count(w) / total_tokens,\n",
    "    # context_counts can store the total number of tokens.\n",
    "    if n == 1:\n",
    "        total_word_occurrences = sum(ngram_counts.values()) # Sum of counts of all unigrams\n",
    "        if total_word_occurrences > 0:\n",
    "            context_counts[()] = total_word_occurrences # Global context for unigrams\n",
    "\n",
    "    print(f\"Custom model training complete. Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Number of unique {n}-grams: {len(ngram_counts)}\")\n",
    "    if n > 1 or (n == 1 and context_counts):\n",
    "        print(f\"Number of unique contexts: {len(context_counts)}\")\n",
    "\n",
    "    # Create a class-like object with the necessary methods for prediction\n",
    "    class CustomNGramModel:\n",
    "        def __init__(self, n, vocab, ngram_counts, context_counts):\n",
    "            self.order = n  # Need 'order' attribute for compatibility with predictor.py\n",
    "            self.n = n\n",
    "            self.vocab = set(vocab)\n",
    "            self.ngram_counts = ngram_counts\n",
    "            self.context_counts = context_counts\n",
    "        \n",
    "        def logscore(self, word, context_tuple=()):\n",
    "            \"\"\"Calculate log probability of word given context.\"\"\"\n",
    "            if not context_tuple and self.n > 1:\n",
    "                # Pad with start symbols if context is empty\n",
    "                context_tuple = tuple([\"<s>\"] * (self.n - 1))\n",
    "            elif len(context_tuple) < (self.n - 1) and self.n > 1:\n",
    "                # Pad context if it's shorter than n-1\n",
    "                context_tuple = tuple([\"<s>\"] * ((self.n - 1) - len(context_tuple))) + context_tuple\n",
    "            \n",
    "            # For lower-order n-grams, we only use the last (n-1) tokens of the context\n",
    "            if self.n > 1 and len(context_tuple) > (self.n - 1):\n",
    "                context_tuple = context_tuple[-(self.n - 1):]\n",
    "            \n",
    "            # Calculate the ngram and context\n",
    "            ngram = context_tuple + (word,)\n",
    "            \n",
    "            # Get counts\n",
    "            ngram_count = self.ngram_counts.get(ngram, 0)\n",
    "            context_count = self.context_counts.get(context_tuple, 0)\n",
    "            \n",
    "            # Calculate probability with smoothing (add-1 smoothing)\n",
    "            # This is a simple version of smoothing, could be improved for better results\n",
    "            smooth_value = 0.1  # Small value for smoothing\n",
    "            if ngram_count == 0:\n",
    "                prob = smooth_value / (context_count + (smooth_value * len(self.vocab)))\n",
    "            else:\n",
    "                prob = (ngram_count + smooth_value) / (context_count + (smooth_value * len(self.vocab)))\n",
    "            \n",
    "            # Return log probability, handle zero probability\n",
    "            import math\n",
    "            if prob <= 0:\n",
    "                return -float('inf')\n",
    "            return math.log(prob)\n",
    "    \n",
    "    # Create and return the model instance\n",
    "    return CustomNGramModel(n, vocab, dict(ngram_counts), dict(context_counts))\n",
    "\n",
    "def save_model(model, model_dir: str = MODEL_DIR, filename: str = DEFAULT_MODEL_FILENAME):\n",
    "    \"\"\"Save trained model to file.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model is None. Nothing to save.\")\n",
    "        return False\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, filename)\n",
    "    print(f\"Saving model to {model_path}...\")\n",
    "    try:\n",
    "        with open(model_path, 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "        print(f\"Model successfully saved to {model_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {model_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    \"\"\"Load trained model from file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERROR: Model file not found at {model_path}\")\n",
    "        print(f\"Please train and save the model first or provide a valid path.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Attempting to load model from {model_path}...\")\n",
    "    try:\n",
    "        with open(model_path, 'rb') as fin:\n",
    "            model_loaded = pickle.load(fin)\n",
    "        # Check if the loaded model has the necessary attributes\n",
    "        if hasattr(model_loaded, 'vocab') and hasattr(model_loaded, 'order'):\n",
    "            print(f\"Model loaded successfully from {model_path}.\")\n",
    "            if hasattr(model_loaded, 'vocab') and isinstance(model_loaded.vocab, set):\n",
    "                print(f\"Vocabulary size: {len(model_loaded.vocab)}\")\n",
    "        else:\n",
    "            print(f\"Model loaded from {model_path}, but it does not have the expected attributes.\")\n",
    "        return model_loaded\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: An error occurred while loading the model from {model_path}. Details: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ef932",
   "metadata": {},
   "source": [
    "## 5. Predictor Module\n",
    "\n",
    "This section implements the accent prediction functionality from `predictor.py`. It includes the beam search algorithm for finding the most likely accented versions of Vietnamese text without accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244765c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor functions (from predictor.py)\n",
    "\n",
    "_detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "def beam_search_predict_accents(text_no_accents: str, model, k: int = 3, \n",
    "                              syllables_file: str = SYLLABLES_PATH, \n",
    "                              detokenizer=_detokenizer) -> list[tuple[str, float]]:\n",
    "    \"\"\"Predict Vietnamese accents using beam search algorithm.\"\"\"\n",
    "    words = text_no_accents.lower().split()\n",
    "    sequences = [] # Stores list of ([word_sequence], score)\n",
    "\n",
    "    for idx, word_no_accent in enumerate(words):\n",
    "        possible_accented_words = gen_accents_word(word_no_accent, syllables_path=syllables_file)\n",
    "        if not possible_accented_words:\n",
    "            possible_accented_words = {word_no_accent}\n",
    "\n",
    "        if idx == 0:\n",
    "            sequences = [([x], 0.0) for x in possible_accented_words]\n",
    "        else:\n",
    "            all_new_sequences = []\n",
    "            for seq_words, seq_score in sequences:\n",
    "                for next_accented_word in possible_accented_words:\n",
    "                    context = seq_words[-(model.order - 1):] if model.order > 1 else [] \n",
    "                    try:\n",
    "                        score_addition = model.logscore(next_accented_word, tuple(context))\n",
    "                    except Exception as e: \n",
    "                        # print(f\"Logscore error for '{next_accented_word}' with context {context}: {e}. Assigning low score.\")\n",
    "                        score_addition = -float('inf') \n",
    "                        \n",
    "                    new_seq_words = seq_words + [next_accented_word]\n",
    "                    all_new_sequences.append((new_seq_words, seq_score + score_addition))\n",
    "            \n",
    "            all_new_sequences = sorted(all_new_sequences, key=lambda x: x[1], reverse=True)\n",
    "            sequences = all_new_sequences[:k]\n",
    "            if not sequences: \n",
    "                if all_new_sequences:\n",
    "                    sequences = [(all_new_sequences[0][0][:-1] + [word_no_accent], all_new_sequences[0][1] - 1000)] \n",
    "                else:\n",
    "                    return []\n",
    "\n",
    "    results = [(detokenizer.detokenize(seq_words), score) for seq_words, score in sequences]\n",
    "    return results\n",
    "\n",
    "# Test prediction with a simple test model if no model is available yet\n",
    "class TestModel:\n",
    "    def __init__(self):\n",
    "        self.order = 2\n",
    "        self.vocab = {\"test\", \"tiếng\", \"việt\", \"rất\", \"hay\"}\n",
    "    \n",
    "    def logscore(self, word, context_tuple=()):\n",
    "        import random\n",
    "        return random.random() * -10  # Random negative score between 0 and -10\n",
    "\n",
    "# Try prediction with test data\n",
    "print(\"Testing accent prediction with a simple test model:\")\n",
    "test_model = TestModel()\n",
    "test_input = \"tieng viet rat hay\"\n",
    "print(f\"Input: '{test_input}'\")\n",
    "predictions = beam_search_predict_accents(test_input, test_model, k=3)\n",
    "if predictions:\n",
    "    print(\"Top predictions:\")\n",
    "    for i, (sent, score) in enumerate(predictions):\n",
    "        print(f\"{i+1}. '{sent}' (Score: {score:.4f})\")\n",
    "else:\n",
    "    print(\"No predictions returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff2a8e",
   "metadata": {},
   "source": [
    "## 6. Evaluation Module\n",
    "\n",
    "This section implements model evaluation functionality from `evaluate_model.py`. It includes metrics for evaluating the quality of accent predictions compared to ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions (from evaluate_model.py)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def calculate_sentence_accuracy(results: list[dict]) -> float:\n",
    "    \"\"\"Calculate percentage of exactly correct sentences.\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    correct_sentences = 0\n",
    "    for res in results:\n",
    "        if res[\"true_accented\"].strip() == res[\"predicted_accented\"].strip():\n",
    "            correct_sentences += 1\n",
    "    return (correct_sentences / len(results)) * 100\n",
    "\n",
    "def calculate_word_accuracy(results: list[dict]) -> float:\n",
    "    \"\"\"Calculate percentage of correctly predicted words.\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    for res in results:\n",
    "        true_words = res[\"true_accented\"].strip().split()\n",
    "        predicted_words = res[\"predicted_accented\"].strip().split()\n",
    "        len_min = min(len(true_words), len(predicted_words))\n",
    "        for i in range(len_min):\n",
    "            if true_words[i] == predicted_words[i]:\n",
    "                correct_words += 1\n",
    "        total_words += len(true_words)\n",
    "    if total_words == 0: return 0.0\n",
    "    return (correct_words / total_words) * 100\n",
    "\n",
    "def calculate_cer(results: list[dict]) -> float:\n",
    "    \"\"\"Calculate Character Error Rate (CER).\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    total_edit_distance = 0\n",
    "    total_true_chars = 0\n",
    "    for res in results:\n",
    "        true_str = res[\"true_accented\"].strip()\n",
    "        pred_str = res[\"predicted_accented\"].strip()\n",
    "        if not true_str and not pred_str:\n",
    "            dist = 0\n",
    "        elif not true_str:\n",
    "            dist = len(pred_str)\n",
    "        elif not pred_str:\n",
    "            dist = len(true_str)\n",
    "        else:\n",
    "            dist = edit_distance(true_str, pred_str)\n",
    "        total_edit_distance += dist\n",
    "        total_true_chars += len(true_str)\n",
    "    if total_true_chars == 0: return 1.0 if total_edit_distance > 0 else 0.0\n",
    "    return (total_edit_distance / total_true_chars) * 100\n",
    "\n",
    "def display_sample_results(results: list[dict], num_samples: int = 5):\n",
    "    \"\"\"Display sample prediction results.\"\"\"\n",
    "    print(\"\\n--- Sample Predictions ---\")\n",
    "    for i, res in enumerate(results[:num_samples]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Input:     '{res['input_unaccented']}'\")\n",
    "        print(f\"  True:      '{res['true_accented']}'\")\n",
    "        print(f\"  Predicted: '{res['predicted_accented']}'\")\n",
    "        print(\"---\")\n",
    "\n",
    "def plot_metrics(metrics: dict):\n",
    "    \"\"\"Plot evaluation metrics.\"\"\"\n",
    "    names = list(metrics.keys())\n",
    "    values = list(metrics.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(names, values)\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.title('Model Evaluation Metrics')\n",
    "\n",
    "    # Add values on top of bars\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(i, value + 1, f\"{value:.2f}%\", ha='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_set, k=3):\n",
    "    \"\"\"Evaluate a model on test data.\"\"\"\n",
    "    if not model or not test_set:\n",
    "        print(\"Model or test set is empty. Cannot evaluate.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Evaluating model on {len(test_set)} test sentences...\")\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for i, (unaccented_input_str, tokenized_true_accented_sent) in enumerate(tqdm(test_set, desc=\"Evaluating\")):\n",
    "        true_accented_str = _detokenizer.detokenize(tokenized_true_accented_sent)\n",
    "        \n",
    "        # Get top prediction\n",
    "        predictions = beam_search_predict_accents(\n",
    "            text_no_accents=unaccented_input_str,\n",
    "            model=model,\n",
    "            k=k,\n",
    "            syllables_file=SYLLABLES_PATH\n",
    "        )\n",
    "        \n",
    "        predicted_accented_str = predictions[0][0] if predictions else \"\"\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"input_unaccented\": unaccented_input_str,\n",
    "            \"true_accented\": true_accented_str,\n",
    "            \"predicted_accented\": predicted_accented_str\n",
    "        })\n",
    "    \n",
    "    print(f\"Evaluation complete. Processed {len(evaluation_results)} test items.\")\n",
    "    return evaluation_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f2507",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "This section demonstrates how to train a new Vietnamese accent prediction model using the loaded training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a new model\n",
    "\n",
    "# Create a smaller sample corpus for quick demonstration if the corpus is large\n",
    "SAMPLE_SIZE_LIMIT = 5000  # Limit sample size to avoid long training times in this notebook\n",
    "if train_corpus and len(train_corpus) > SAMPLE_SIZE_LIMIT:\n",
    "    print(f\"Using a sample of {SAMPLE_SIZE_LIMIT} sentences from the {len(train_corpus)} available for faster training.\")\n",
    "    random.seed(42)  # For reproducibility\n",
    "    train_sample = random.sample(train_corpus, SAMPLE_SIZE_LIMIT)\n",
    "else:\n",
    "    train_sample = train_corpus\n",
    "\n",
    "print(f\"Training with {len(train_sample)} sentences...\")\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_ngram_model(train_sample, n=N_GRAM_ORDER)\n",
    "\n",
    "if trained_model:\n",
    "    print(\"Model training successful.\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_saved = save_model(trained_model, model_dir=MODEL_DIR, filename=DEFAULT_MODEL_FILENAME)\n",
    "    \n",
    "    if model_saved:\n",
    "        print(f\"Model successfully saved to {os.path.join(MODEL_DIR, DEFAULT_MODEL_FILENAME)}\")\n",
    "    else:\n",
    "        print(\"Failed to save model.\")\n",
    "else:\n",
    "    print(\"Model training failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f880c",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model\n",
    "\n",
    "This section demonstrates how to evaluate the trained Vietnamese accent prediction model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "\n",
    "# Load the model if not already in memory\n",
    "model_path = os.path.join(MODEL_DIR, DEFAULT_MODEL_FILENAME)\n",
    "if not trained_model and os.path.exists(model_path):\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    trained_model = load_model(model_path)\n",
    "\n",
    "if trained_model and test_set:\n",
    "    # Limit test set size for quicker evaluation\n",
    "    MAX_TEST_SAMPLES = 100\n",
    "    if len(test_set) > MAX_TEST_SAMPLES:\n",
    "        print(f\"Using {MAX_TEST_SAMPLES} random samples from test set for quick evaluation.\")\n",
    "        random.seed(42)  # For reproducibility\n",
    "        test_sample = random.sample(test_set, MAX_TEST_SAMPLES)\n",
    "    else:\n",
    "        test_sample = test_set\n",
    "    \n",
    "    print(f\"Evaluating model on {len(test_sample)} test samples...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluation_results = evaluate_model(trained_model, test_sample, k=3)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        # Calculate metrics\n",
    "        sent_accuracy = calculate_sentence_accuracy(evaluation_results)\n",
    "        word_acc = calculate_word_accuracy(evaluation_results)\n",
    "        char_error_rate = calculate_cer(evaluation_results)\n",
    "        char_accuracy = 100.0 - char_error_rate\n",
    "        \n",
    "        # Display metrics\n",
    "        print(f\"\\n--- Evaluation Metrics ---\")\n",
    "        print(f\"Sentence Accuracy: {sent_accuracy:.2f}%\")\n",
    "        print(f\"Word Accuracy: {word_acc:.2f}%\")\n",
    "        print(f\"Character Error Rate (CER): {char_error_rate:.2f}%\")\n",
    "        print(f\"Character Accuracy: {char_accuracy:.2f}%\")\n",
    "        \n",
    "        # Plot metrics\n",
    "        metrics_to_plot = {\n",
    "            \"Sentence Accuracy\": sent_accuracy,\n",
    "            \"Word Accuracy\": word_acc,\n",
    "            \"Character Accuracy\": char_accuracy\n",
    "        }\n",
    "        plot_metrics(metrics_to_plot)\n",
    "        \n",
    "        # Show sample results\n",
    "        display_sample_results(evaluation_results, num_samples=5)\n",
    "    else:\n",
    "        print(\"No evaluation results available.\")\n",
    "else:\n",
    "    if not trained_model:\n",
    "        print(\"No trained model available for evaluation.\")\n",
    "    if not test_set:\n",
    "        print(\"No test data available for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbc640",
   "metadata": {},
   "source": [
    "## 9. Interactive Accent Prediction\n",
    "\n",
    "This section provides an interactive interface to predict accents in Vietnamese text. Enter text without accents and see the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9c0cf",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6eec29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Load the model if not already in memory\n",
    "model_path = os.path.join(MODEL_DIR, DEFAULT_MODEL_FILENAME)\n",
    "if not trained_model and os.path.exists(model_path):\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    trained_model = load_model(model_path)\n",
    "\n",
    "if not trained_model:\n",
    "    print(\"No model available for prediction. Please train a model first.\")\n",
    "else:\n",
    "    # Create widgets for interactive prediction\n",
    "    input_text = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Nhập câu tiếng Việt không dấu ở đây...',\n",
    "        description='Input:',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%', 'height': '80px'}\n",
    "    )\n",
    "\n",
    "    beam_width = widgets.IntSlider(\n",
    "        value=3,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description='Beam Width (k):',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def on_predict_button_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            if not input_text.value.strip():\n",
    "                print(\"Please enter some text.\")\n",
    "                return\n",
    "            \n",
    "            text_no_accents = input_text.value.strip()\n",
    "            print(f\"Input: '{text_no_accents}'\")\n",
    "            \n",
    "            predictions = beam_search_predict_accents(\n",
    "                text_no_accents=text_no_accents,\n",
    "                model=trained_model,\n",
    "                k=beam_width.value,\n",
    "                syllables_file=SYLLABLES_PATH\n",
    "            )\n",
    "            \n",
    "            if predictions:\n",
    "                print(\"\\nPredictions:\")\n",
    "                for i, (sent, score) in enumerate(predictions):\n",
    "                    print(f\"{i+1}. '{sent}' (Score: {score:.4f})\")\n",
    "            else:\n",
    "                print(\"No predictions returned.\")\n",
    "\n",
    "    predict_button = widgets.Button(\n",
    "        description='Predict Accents',\n",
    "        disabled=False,\n",
    "        button_style='primary',\n",
    "        tooltip='Click to predict accents',\n",
    "        icon='check'\n",
    "    )\n",
    "    predict_button.on_click(on_predict_button_clicked)\n",
    "\n",
    "    # Display the interactive elements\n",
    "    display(widgets.HTML('<h3>Vietnamese Accent Prediction</h3>'))\n",
    "    display(input_text)\n",
    "    display(beam_width)\n",
    "    display(predict_button)\n",
    "    display(output_area)\n",
    "\n",
    "    # Add some example sentences that users can try\n",
    "    display(widgets.HTML('<h4>Example Sentences to Try:</h4>'))\n",
    "    examples = [\n",
    "        \"toi la nguoi viet nam\",\n",
    "        \"chuc mung nam moi\",\n",
    "        \"hom nay troi dep qua\",\n",
    "        \"tieng viet rat hay\",\n",
    "        \"ban dang o dau\"\n",
    "    ]\n",
    "    for example in examples:\n",
    "        display(widgets.HTML(f\"<code>{example}</code>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d387bb",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "This notebook has implemented a complete Vietnamese accent prediction model that can:\n",
    "\n",
    "1. Process Vietnamese text and remove/add accents\n",
    "2. Load and prepare corpus data\n",
    "3. Train n-gram language models\n",
    "4. Predict accents using beam search\n",
    "5. Evaluate model performance with multiple metrics\n",
    "6. Provide an interactive interface for predictions\n",
    "\n",
    "### Possible Improvements\n",
    "\n",
    "- **Data Augmentation**: Expand training data with more diverse Vietnamese text\n",
    "- **Advanced Models**: Implement neural models (LSTM, Transformer) for better prediction\n",
    "- **Smoothing Techniques**: Implement more sophisticated smoothing methods for n-gram models\n",
    "- **User Interface**: Build a web application or mobile app for easier access\n",
    "- **Contextual Analysis**: Consider broader context beyond n-grams for better disambiguation\n",
    "\n",
    "### References\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Vietnamese Language Resources](https://github.com/undertheseanlp/resources)\n",
    "- [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
